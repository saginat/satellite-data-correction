{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c79df200",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T13:09:30.132217Z",
     "start_time": "2023-09-12T13:09:21.741088Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labs/rudich/sagima/.conda/envs/sagi_base/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/labs/rudich/sagima/.conda/envs/sagi_base/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange\n",
    "#from torchsummary import summary\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../helpers')\n",
    "# sys.path.insert(0,'/home/labs/rudich/sagima/helpers')\n",
    "from plotting import *\n",
    "from validate import *\n",
    "from models import *\n",
    "from data_handlers import *\n",
    "from training import *\n",
    "from metrics import *\n",
    "from explore import *\n",
    "from torchsummary import *\n",
    "from visual_helpers_2 import *\n",
    "\n",
    "#from visual_helpers import *\n",
    "\n",
    "import itertools\n",
    "\n",
    "import json\n",
    "\n",
    "#taken from the model building section - \n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 800)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "import xgboost as xg\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "import scipy \n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import torch.optim as optim\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy.fft import fft, fftfreq\n",
    "import folium\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.patches import Rectangle\n",
    "import cartopy.feature\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "\n",
    "import ast\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4c5526a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T13:09:32.232514Z",
     "start_time": "2023-09-12T13:09:32.217590Z"
    }
   },
   "outputs": [],
   "source": [
    "df_cord = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/coordinates_pm.csv\")\n",
    "df_cord = df_cord.rename(columns={df_cord.columns[0]:'station_id'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac5a420",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T13:25:58.837417Z",
     "start_time": "2023-09-12T13:25:55.745572Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pm0 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_0.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm1 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_1.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm2 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_2.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm3 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_3.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm4 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_4.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm5 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_5.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm6 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_6.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "\n",
    "df_pm =(\n",
    "        pd.merge(df_pm0, df_pm1, how='outer',left_index=True, right_index=True)\n",
    "        .pipe(lambda d:pd.merge(d, df_pm2, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm3, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm4, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm5, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm6, how='outer',left_index=True, right_index=True))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c172653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_string_to_tuple_list(string_list):\n",
    "    tuple_list = []\n",
    "    for string in string_list:\n",
    "        # Remove leading and trailing whitespace and parentheses\n",
    "        string = string.strip('()')\n",
    "        # Convert string to tuple using ast.literal_eval\n",
    "        tuple_value = ast.literal_eval(string)\n",
    "        tuple_list.append(tuple_value)\n",
    "    return tuple_list\n",
    "\n",
    "with open(\"saved_turkey_lat_lon_cords\", \"rb\") as fp:   # Unpickling\n",
    "    saved_turkey_lat_lon_cords = pickle.load(fp)\n",
    "\n",
    "with open(\"saved_turkey_lat_lon_idx\", \"rb\") as fp:   # Unpickling\n",
    "    saved_turkey_lat_lon_idx = pickle.load(fp)\n",
    "    \n",
    "with open(\"saved_greece_lat_lon_cords\", \"rb\") as fp:   # Unpickling\n",
    "    saved_greece_lat_lon_cords= pickle.load(fp)\n",
    "\n",
    "with open(\"saved_greece_lat_lon_idx\", \"rb\") as fp:   # Unpickling\n",
    "    saved_greece_lat_lon_idx = pickle.load(fp)\n",
    "    \n",
    "saved_turkey_lat_lon_cords = convert_string_to_tuple_list(saved_turkey_lat_lon_cords)\n",
    "saved_turkey_lat_lon_cords.append((35.0,27.0))\n",
    "saved_turkey_lat_lon_cords.append((35.0, 45.5))\n",
    "\n",
    "with open(\"saved_turkey_and_greece_lat_lon_cords\", \"rb\") as fp:   # Unpickling\n",
    "    saved_turkey_and_greece_lat_lon_cords= pickle.load(fp)\n",
    "\n",
    "with open(\"saved_turkey_and_greece_lat_lon_idx\", \"rb\") as fp:   # Unpickling\n",
    "    saved_turkey_and_greece_lat_lon_idx = pickle.load(fp)\n",
    "\n",
    "with open(\"grecee_pixels_stations_dict_subset.pkl\", \"rb\") as fp:   # Unpickling\n",
    "    greecee_subset_stations_dict = pickle.load(fp)\n",
    "with open(\"grecee_pixels_stations_dict_subset_NN_stations.pkl\", \"rb\") as fp:   # Unpickling\n",
    "    greecee_subset_stations_dict = pickle.load(fp)\n",
    "with open(\"stations_to_avoid_in_case_of_clash.json\", \"rb\") as fp:   # Unpickling\n",
    "    bad_stations_dict = json.load(fp)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b88e6fce-a44e-41e9-8a1a-1d8b0a82cdd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28218]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turkey_subset_stations_dict.pop('(34.5, 33.0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54b94e85-7cce-46d5-8d72-7c3cd11a470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_keys = []\n",
    "for x in list(greecee_subset_stations_dict.keys()):\n",
    "    x = x[1:-1]\n",
    "    l = x.split()\n",
    "    lat = float(l[0])\n",
    "    lon = float(l[1])\n",
    "    new_keys.append((lat, lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7514b42f-e6fa-4a36-b2c6-44aed1d44f18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dict = {new_key: greecee_subset_stations_dict[old_key] for old_key, new_key in zip(greecee_subset_stations_dict.keys(), new_keys)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef56871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ls /home/labs/rudich/Collaboration/dust_prediction/data/interpolated_mean_normalized/\n",
    "\n",
    "# Dnote:\n",
    "\n",
    "\n",
    "\n",
    "colab_dir = \"/home/labs/rudich/Collaboration/dust_prediction/data\"\n",
    "meteo_dir = colab_dir+\"/interpolated_mean_normalized/\"\n",
    "meteo_de_dir = colab_dir+\"/meteorology_renormalization_tensors\"\n",
    "dust_dir = colab_dir+\"/dust_61368_108_2_339/\"\n",
    "\n",
    "pm10_all = torch.load(meteo_de_dir+\"/meteorology_tensors_1_81_189_interpolated_mean_normalized_pm10_tensor_denormalized.pkl\")\n",
    "labels_all = torch.load(dust_dir+\"dust_61368_108_2_339_full_tensor.pkl\")\n",
    "\n",
    "timestamps_labels = torch.load(dust_dir+\"dust_61368_108_2_339_full_timestamps.pkl\")\n",
    "timestamps_mpm = pd.to_datetime(torch.load(colab_dir+\"/meteorology_tensors_1_81_189_general_timestamps.pkl\"))\n",
    "timestamps_all = pd.to_datetime(torch.load(colab_dir+\"/meteorology_tensors_1_81_189_general_timestamps.pkl\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ab2533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "colab_dir = \"/home/labs/rudich/Collaboration/dust_prediction/data\"\n",
    "meteo_dir = colab_dir+\"/interpolated_mean_normalized/\"\n",
    "meteo_de_dir = colab_dir+\"/meteorology_renormalization_tensors\"\n",
    "dust_dir = colab_dir+\"/dust_61368_108_2_339/\"\n",
    "\n",
    "meteo_meta = torch.load(meteo_dir+\"/meteorology_tensor_62_81_189_metadata.pkl\")\n",
    "dust_meta = torch.load(dust_dir+\"/dust_61368_108_2_339_metadata.pkl\")\n",
    "\n",
    "lons_dict = meteo_meta['dims']['lons']\n",
    "lats_dict = meteo_meta['dims']['lats']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f954f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29387/2475775844.py:26: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n",
      "  timestamps = df_pm.index.values.astype('datetime64[s]').astype(np.int64)\n",
      " 50%|██████████████████████████████████████████▌                                          | 1/2 [00:00<00:00,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_pm0 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_0.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm1 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_1.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm2 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_2.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm3 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_3.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm4 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_4.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm5 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_5.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm6 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_6.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "\n",
    "df_pm =(\n",
    "        pd.merge(df_pm0, df_pm1, how='outer',left_index=True, right_index=True)\n",
    "        .pipe(lambda d:pd.merge(d, df_pm2, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm3, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm4, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm5, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm6, how='outer',left_index=True, right_index=True))\n",
    "\n",
    "\n",
    ")\n",
    "df_cord = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/coordinates_pm.csv\")\n",
    "df_cord = df_cord.rename(columns={df_cord.columns[0]:'station_id'})\n",
    "\n",
    "\n",
    "first_year = int(df_pm.index[0][:4])\n",
    "last_year = int(df_pm.index[-1][:4])\n",
    "\n",
    "timestamps = df_pm.index.values.astype('datetime64[s]').astype(np.int64)\n",
    "stn_ids = df_pm.columns.values.astype(np.int64)\n",
    "\n",
    "\n",
    "pm10_values = df_pm.to_numpy()\n",
    "\n",
    "pm10_tensor = pm10_values.reshape(len(timestamps), len(stn_ids), 1)\n",
    "\n",
    "# Convert the tensor to a PyTorch tensor\n",
    "labels_all = torch.from_numpy(pm10_tensor)\n",
    "\n",
    "timestamp_labels = pd.to_datetime(df_pm.index)\n",
    "\n",
    "zero_var = (pm10_all.var((2,3))==0).reshape(-1).numpy()\n",
    "pm10_valid_timestamps = timestamps_all[~zero_var]\n",
    "pm10_valid_all = pm10_all[~zero_var]\n",
    "\n",
    "l = [pm10_valid_timestamps, timestamp_labels]\n",
    "shared_timestamps = l[0].intersection(l[1])\n",
    "\n",
    "shared_timestamps_idxs = []\n",
    "for timestamps_list in tqdm(l):\n",
    "    shared_idxs_per_list = []\n",
    "    for shared_t in shared_timestamps:\n",
    "        try:\n",
    "            shared_idxs_per_list.append(timestamps_list.get_loc(shared_t))\n",
    "            \n",
    "        except:\n",
    "            print(f\"Error! Something wierd happend with {shared_t} and {timestamps_list}. Aborting\")\n",
    "        if shared_t == shared_timestamps[-1]:\n",
    "            print(max(shared_idxs_per_list))\n",
    "    shared_timestamps_idxs.append(shared_idxs_per_list)\n",
    "\n",
    "inputs_all = pm10_valid_all\n",
    "labels_all = labels_all\n",
    "\n",
    "\n",
    "\n",
    "years = [ i for i in range(first_year, last_year)]\n",
    "months = [i for i in range(1,13)]\n",
    "\n",
    "dt = pd.Series(shared_timestamps).dt\n",
    "years = np.isin(dt.year, years)\n",
    "months = np.isin(dt.month, months)\n",
    "include = (years) * (months)\n",
    "\n",
    "pm10_shared_timestamps = shared_timestamps[include]\n",
    "inputs = inputs_all[shared_timestamps_idxs[0]][include]\n",
    "labels = labels_all[shared_timestamps_idxs[1]][include]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b1b7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels46 = [\"Q250\",\"Q500\",\"Q700\",\"Q850\",\"Q900\",\n",
    "            \"T250\",\"T500\",\"T700\",\"T850\",\"T900\",\n",
    "            \"U250\",\"U500\",\"U700\",\"U850\",\"U900\",\n",
    "            \"V250\",\"V500\",\"V700\",\"V850\",\"V900\",\n",
    "            \"Z250\",\"Z500\",\"Z700\",\"Z850\",\"Z900\",\n",
    "            \"PV250\",\"PV500\",\"PV700\",\"PV850\",\"PV900\",\n",
    "            \"OMEGA250\",\"OMEGA500\",\"OMEGA700\",\"OMEGA850\",\"OMEGA900\",\n",
    "            \"SLP\",\"u10\",\"v10\",\"duaod550\",'aermr06_50','aermr06_40','aermr06_30', 'aermr06_20',\n",
    "              \"pm2p5\",\"pm10\",\"tcwv\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbd0f747-7fc7-4769-8eee-b94be3d0d242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the slice indices\n",
    "lat_start, lat_end = 10, 75\n",
    "lon_start, lon_end = 50, 117\n",
    "\n",
    "# Create new dictionaries for the sliced lat and lon indices\n",
    "new_lats_dict = {i: lats_dict[i] for i in range(lat_start, lat_end)}\n",
    "new_lons_dict = {i: lons_dict[i] for i in range(lon_start, lon_end)}\n",
    "\n",
    "def replace_keys_with_sequential_integers(dictionary):\n",
    "    new_dict = {}\n",
    "    keys = list(dictionary.keys())\n",
    "    for i in range(len(keys)):\n",
    "        new_dict[i] = dictionary[keys[i]]\n",
    "    return new_dict\n",
    "\n",
    "new_lats_dict = replace_keys_with_sequential_integers(new_lats_dict)\n",
    "new_lons_dict = replace_keys_with_sequential_integers(new_lons_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4af3314a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 46/46 [01:39<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52514, 46, 15, 37])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tensors_list = []\n",
    "pixels_list = []\n",
    "#labels_all = torch.load(dust_dir+\"dust_61368_108_2_339_full_tensor.pkl\")\n",
    "for channel in tqdm(range(len(channels46))):\n",
    "    data_dir = \"/home/labs/rudich/Collaboration/dust_prediction/data/interpolated_mean_normalized/\"\n",
    "    dust_dir = \"/home/labs/rudich/Collaboration/dust_prediction/data/dust_61368_108_2_339/\"\n",
    "    channel_all = torch.load(f\"/home/labs/rudich/Collaboration/dust_prediction/data/meteorology_renormalization_tensors/meteorology_tensors_1_81_189_interpolated_mean_normalized_{channels46[channel]}_tensor_denormalized.pkl\")\n",
    "    subset_channel = channel_all[:, :, lat_start:lat_end, lon_start:lon_end]\n",
    "    tensors_list.append(subset_channel)\n",
    "stacked_tensor = torch.stack(tensors_list, dim=1)\n",
    "stacked_tensor = stacked_tensor.squeeze(dim=2)\n",
    "stacked_tensor = stacked_tensor[~zero_var]\n",
    "print(stacked_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2235aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stations_lonlat = (df_cord \n",
    "                       .rename(columns={'lng':'lon'})\n",
    "                       .assign(stn_id=np.arange(0,df_cord.shape[0],1))\n",
    "                        )\n",
    "station_df = all_stations_lonlat\n",
    "pm10_df = df_pm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9285a30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7991cab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "stations_tensor = np.zeros((pm10_valid_timestamps.shape[0], 1, len(new_lats_dict), len(new_lons_dict)))\n",
    "\n",
    "inv_lats = {v: k for k, v in new_lats_dict.items()}\n",
    "inv_lons = {v: k for k, v in new_lons_dict.items()}\n",
    "list_of_pixels_with_stations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9fe1cf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pm10_df.index = pd.to_datetime(pm10_df.index)\n",
    "pm10_df.columns =  pm10_df.columns.astype(int)\n",
    "set1 = set(pm10_valid_timestamps)\n",
    "set2 = set(pm10_df.index)\n",
    "\n",
    "# Find the intersection of the sets\n",
    "intersection = set1.intersection(set2)\n",
    "\n",
    "# Convert the intersection set to a DatetimeIndex\n",
    "common_timestamps = pd.DatetimeIndex(pd.to_datetime(list(intersection)))\n",
    "common_timestamps = common_timestamps.sort_values()\n",
    "# Find the index where the intersection starts\n",
    "start_index = list(pm10_valid_timestamps).index(common_timestamps[0])\n",
    "\n",
    "# Find the index where the intersection ends\n",
    "end_index = list(pm10_valid_timestamps).index(common_timestamps[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38d29435-c3c5-46df-bf72-03e4c8e9600d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[56722]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dict.pop((41.5, 26.5))\n",
    "new_dict.pop((41.0, 26.5))\n",
    "new_dict.pop((40.0, 26.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e7bca9",
   "metadata": {},
   "source": [
    "# Creating a dict with pixel coordinates - stations id - distances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c87fe770-94de-4ad0-9d44-b616eee4cd9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 59.86it/s]\n"
     ]
    }
   ],
   "source": [
    "stations_tensor[:, 0, :, :] = np.nan\n",
    "\n",
    "mean_values = np.zeros(stations_tensor.shape[:3])\n",
    "\n",
    "\n",
    "for key in tqdm(new_dict):\n",
    "    lat = key[0]\n",
    "    lon = key[1]\n",
    "\n",
    "    lat_index = inv_lats[lat]\n",
    "    lon_index = inv_lons[lon]\n",
    "\n",
    "    df_rows = pm10_df.loc[common_timestamps, new_dict[key]]\n",
    "\n",
    "    mean_values = df_rows.mean(axis=1) \n",
    "\n",
    "    stations_tensor[:, 0, lat_index, lon_index] = mean_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "72fe4519",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 118/118 [1:11:23<00:00, 36.30s/it]\n"
     ]
    }
   ],
   "source": [
    "stations_tensor[:,0,:,:] = np.nan\n",
    "l = list(bad_stations_dict.values())\n",
    "bad_stations = [item for sublist in l for item in sublist]\n",
    "\n",
    "for key in tqdm(turkey_subset_stations_dict):\n",
    "\n",
    "    lat = key[0]\n",
    "    lon = key[1]\n",
    "\n",
    "    # Get the corresponding indices from the inv_lats and inv_lons dictionaries\n",
    "    lat_index = inv_lats[lat]\n",
    "    lon_index = inv_lons[lon]\n",
    "\n",
    "    for t, timestamp in enumerate(common_timestamps):\n",
    "        if t >= start_index:\n",
    "            \n",
    "            df_row = pm10_df.loc[timestamp]\n",
    "            series_helper = df_row.loc[turkey_subset_stations_dict[key]].dropna()\n",
    "            filtered_df = df_row[[element for element in series_helper.index if element not in bad_stations]]\n",
    "            mean_value = filtered_df.mean()\n",
    "            stations_tensor[t, 0, lat_index, lon_index] = mean_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "05bb67ec-78bd-424c-b393-b51019a6c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_check = np.isnan(tensor_check)\n",
    "# mask_stations = np.isnan(stations_tensor)\n",
    "\n",
    "# # Replace NaN values with zeros (or any other value)\n",
    "# tensor_check[mask_check] = 0\n",
    "# stations_tensor[mask_stations] = 0\n",
    "\n",
    "# # Calculate the Euclidean distance\n",
    "# euclidean_distance = np.linalg.norm(tensor_check - stations_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "14ed37b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stations_tensor = torch.load('stations_fixed3.pt')\n",
    "# torch.save(torch.from_numpy(stations_tensor), 'stations_fixed3.pt')\n",
    "# full = torch.cat((stacked_tensor,torch.from_numpy(stations_tensor)),dim=1)\n",
    "# full = torch.cat((stacked_tensor,stations_tensor),dim=1)\n",
    "# del stacked_tensor\n",
    "# del stations_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21df7199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(sliced_tensor, 'new_turkey_greece_tensor_fixed.pt')\n",
    "# sliced_tensor = torch.load('new_turkey_greece_tensor_fixed.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a1f32708-95c5-4dea-b137-fdf62148fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torch.tensor(stations_tensor),'turkey_stations_tensor_subset_ver3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "075764f4-73ad-45ea-b1d9-998cf2e69f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_tensor = torch.load('turkey_stations_tensor_subset.pt')\n",
    "full = torch.cat((stacked_tensor,stations_tensor),dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bd4c224-a40a-4df0-b4f8-17b7a8a470de",
   "metadata": {},
   "outputs": [],
   "source": [
    "del stacked_tensor\n",
    "del stations_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "dd6cc994-9f9f-477e-9aa5-435dc1a2b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full = torch.cat((stacked_tensor,torch.tensor(stations_tensor)),dim=1)\n",
    "del stacked_tensor\n",
    "del stations_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a612f9b5-7e74-49cb-ad87-988caee19d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(full, 'turkey_subset_stations_full_tensor_ver2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42f12285-5060-4468-82bb-4ea6f9dc4e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "full = torch.load('turkey_subset_stations_full_tensor_ver2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc83b272-3077-4e24-a885-a8be286cb16c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52514, 47, 15, 37])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01276567-6ad7-4b10-bd7c-02b32a7e2f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52514, 1, 65, 67)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bfb82e",
   "metadata": {},
   "source": [
    "# tensor to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6ca6301",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 65/65 [00:06<00:00,  9.38it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tensor = stations_tensor\n",
    "\n",
    "tensor_shape = (52514, 1, 65, 67)\n",
    "\n",
    "timestamps = common_timestamps # List of timestamps (length 52514)\n",
    "\n",
    "\n",
    "reshaped_tensor = np.reshape(tensor, (tensor_shape[0], tensor_shape[1], -1))\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for lat_index in tqdm(range(tensor_shape[2])):\n",
    "    for lon_index in range(tensor_shape[3]):        \n",
    "\n",
    "        df = pd.DataFrame(reshaped_tensor[:, :, (lat_index * tensor_shape[3]) + lon_index])\n",
    "\n",
    "        df['time'] = timestamps\n",
    "        lat_loc = new_lats_dict[lat_index]\n",
    "        lon_loc = new_lons_dict[lon_index]\n",
    "        df['lat_lon_coord'] = str((float(lat_loc),float(lon_loc)))\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "del dfs\n",
    "del tensor\n",
    "# del full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "336f1636-26fb-42d7-ba45-6f868e067e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_axis(['stn_mesm','time','lat_lon_coord'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b742cd07-803f-4a57-b224-d1fad4f05db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('plot_helper_nn_stations.parquet.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bdbf055f-3fa9-403c-bf88-4793480f7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[0]].to_parquet('fixed_stations_column.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2bd9d343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "df = (\n",
    "       df\n",
    "      .set_axis(channels46 +['stn_mesm','time','lat_lon_coord'],axis=1 )\n",
    "      .drop('pm2p5',axis=1)\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af27379f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped = df.groupby('lat_lon_coord')\n",
    "\n",
    "# Creating new columns with previous row values for each group\n",
    "for column in df.drop(['stn_mesm','time','lat_lon_coord'],axis=1).columns:\n",
    "    new_column_name = column + '_prev'\n",
    "    df[new_column_name] = grouped[column].shift()\n",
    "    \n",
    "df['week'] = df['time'].dt.strftime('%W')\n",
    "df['hour'] = df['time'].dt.hour\n",
    "df['month'] = df['time'].dt.month\n",
    "df['year'] = df['time'].dt.year\n",
    "df['sin'] = np.sin(df['time'].dt.dayofyear * 2 * np.pi / 365)\n",
    "df['cos'] = np.cos(df['time'].dt.dayofyear * 2 * np.pi / 365)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81def6ba-5ef8-4bf4-9dcc-2cb157adb733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29145270, 49)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "136f12f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_delta_columns(df):\n",
    "    channels_used =  [\"Q250\",\"Q500\",\"Q700\",\"Q850\",\"Q900\",\n",
    "            \"T250\",\"T500\",\"T700\",\"T850\",\"T900\",\n",
    "            \"U250\",\"U500\",\"U700\",\"U850\",\"U900\",\n",
    "            \"V250\",\"V500\",\"V700\",\"V850\",\"V900\",\n",
    "            \"Z250\",\"Z500\",\"Z700\",\"Z850\",\"Z900\",\n",
    "            \"PV250\",\"PV500\",\"PV700\",\"PV850\",\"PV900\",\n",
    "            \"OMEGA250\",\"OMEGA500\",\"OMEGA700\",\"OMEGA850\",\"OMEGA900\",\n",
    "            \"SLP\",\"u10\",\"v10\",\"duaod550\",'aermr06_50','aermr06_40','aermr06_30', 'aermr06_20',\n",
    "              \"tcwv\"]\n",
    "\n",
    "    # Loop over the channels names\n",
    "    for col in channels_used:\n",
    "        \n",
    "        # Calculate the delta between the column and its corresponding \"_prev\" column\n",
    "        delta_col = col + '_delta'\n",
    "        prev_col = col + '_prev'\n",
    "        df[delta_col] = df[col] - df[prev_col]\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = add_delta_columns(df)\n",
    "df = df.drop(['pm10_prev'],axis=1)\n",
    "df['pm10'] = df['pm10'] * (10**9)\n",
    "df['target'] = df['stn_mesm'] - df['pm10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab2713dc-9e5a-448f-b9ad-9c525a638651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q250</th>\n",
       "      <th>Q500</th>\n",
       "      <th>Q700</th>\n",
       "      <th>Q850</th>\n",
       "      <th>Q900</th>\n",
       "      <th>T250</th>\n",
       "      <th>T500</th>\n",
       "      <th>T700</th>\n",
       "      <th>T850</th>\n",
       "      <th>T900</th>\n",
       "      <th>...</th>\n",
       "      <th>SLP_delta</th>\n",
       "      <th>u10_delta</th>\n",
       "      <th>v10_delta</th>\n",
       "      <th>duaod550_delta</th>\n",
       "      <th>aermr06_50_delta</th>\n",
       "      <th>aermr06_40_delta</th>\n",
       "      <th>aermr06_30_delta</th>\n",
       "      <th>aermr06_20_delta</th>\n",
       "      <th>tcwv_delta</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.002609</td>\n",
       "      <td>0.005352</td>\n",
       "      <td>0.006493</td>\n",
       "      <td>220.551987</td>\n",
       "      <td>256.688263</td>\n",
       "      <td>272.292419</td>\n",
       "      <td>279.520905</td>\n",
       "      <td>282.570587</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-16.577514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.003010</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>219.105560</td>\n",
       "      <td>255.949677</td>\n",
       "      <td>272.005768</td>\n",
       "      <td>278.810516</td>\n",
       "      <td>282.191406</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.328796</td>\n",
       "      <td>0.420458</td>\n",
       "      <td>0.003151</td>\n",
       "      <td>0.02826</td>\n",
       "      <td>5.319999e-09</td>\n",
       "      <td>3.791060e-11</td>\n",
       "      <td>-3.630541e-12</td>\n",
       "      <td>-1.508498e-12</td>\n",
       "      <td>-1.56883</td>\n",
       "      <td>-32.734256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Q250      Q500      Q700      Q850      Q900        T250        T500  \\\n",
       "0  0.000016  0.000148  0.002609  0.005352  0.006493  220.551987  256.688263   \n",
       "1  0.000013  0.000539  0.001013  0.003010  0.004505  219.105560  255.949677   \n",
       "\n",
       "         T700        T850        T900  ...  SLP_delta  u10_delta  v10_delta  \\\n",
       "0  272.292419  279.520905  282.570587  ...        NaN        NaN        NaN   \n",
       "1  272.005768  278.810516  282.191406  ...  -1.328796   0.420458   0.003151   \n",
       "\n",
       "   duaod550_delta  aermr06_50_delta  aermr06_40_delta  aermr06_30_delta  \\\n",
       "0             NaN               NaN               NaN               NaN   \n",
       "1         0.02826      5.319999e-09      3.791060e-11     -3.630541e-12   \n",
       "\n",
       "   aermr06_20_delta  tcwv_delta     target  \n",
       "0               NaN         NaN -16.577514  \n",
       "1     -1.508498e-12    -1.56883 -32.734256  \n",
       "\n",
       "[2 rows x 143 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b90904b-7e22-490e-b77f-2057b8c30ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = df['time'].sort_values().unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5947bca2-a991-483a-8a4a-0f35a3d6d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('timestamps_turkey_subset.pkl','wb') as f:\n",
    "    pickle.dump(t,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ae12941-48a8-45bf-a399-0d3adf0747ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a =( \n",
    "    pd.read_parquet('turkey_ndvi_subset_stations.parquet.gzip')\n",
    "    .set_axis(['NDVI', 'time','lat_lon_coord'],axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c94cbe7-75a5-4a9b-b596-bf02ac84623d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29145270, 143), (29145270, 3))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98ceb550-41e1-4feb-a5ef-57dc0e00f49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NDVI'] = a['NDVI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10fca6af-b53a-4cca-8454-6d23366d8b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('df_turkey_subset_stations_ver2.parquet.gzip',\n",
    "              compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b1d509b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# del sliced_tensor\n",
    "# del tensor\n",
    "# df.to_csv('turkey_greece_fixed.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagi_base",
   "language": "python",
   "name": "sagi_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
